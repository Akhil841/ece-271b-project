{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "comments = []\n",
    "decoder = json.JSONDecoder()\n",
    "\n",
    "with open(\"reddit_comments_dec_2024.json\", \"r\") as f:\n",
    "    data = f.read()  # Read the entire file as a string\n",
    "    pos = 0\n",
    "    while pos < len(data):\n",
    "        try:\n",
    "            obj, index = decoder.raw_decode(data, pos)  # Decode a JSON object\n",
    "            comments.append(obj)\n",
    "            pos = index  # Move the position forward\n",
    "        except json.JSONDecodeError:\n",
    "            break  # Stop if there's an error in decoding\n",
    "\n",
    "comments = comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set([comment[\"author\"] for comment in comments]))\n",
    "# print(len(classes))\n",
    "\n",
    "# get test and train data\n",
    "import random\n",
    "random.shuffle(comments)\n",
    "train_data = comments[:int(len(comments)*0.8)]\n",
    "test_data = comments[int(len(comments)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comments[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort comments by comment body length and get number of words in longest comment\n",
    "a = sorted(train_data, key=lambda x: len(x[\"body\"]))\n",
    "max_comment_length = len(a[-1][\"body\"].split())\n",
    "print(max_comment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = list(set([comment[\"subreddit\"] for comment in comments]))\n",
    "print(len(subs))\n",
    "\n",
    "class_indices = {c: i for i, c in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2000)  # You can limit the features\n",
    "\n",
    "comment_bodies = [comment[\"body\"] for comment in (train_data + test_data)]\n",
    "\n",
    "vectorizer.fit(comment_bodies)\n",
    "\n",
    "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    import pickle\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vectorizer.transform([\"the world and\"]).toarray().flatten()\n",
    "print(sum(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SUB_FEATURES = 1024\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def feature(comment):\n",
    "    create_time = comment[\"created_utc\"]\n",
    "    score = comment[\"score\"]\n",
    "    ups = comment[\"ups\"]\n",
    "    sub_encoding = [0] * NUM_SUB_FEATURES\n",
    "    sub_index = int(hashlib.sha1(comment[\"subreddit\"].encode()).hexdigest(), 16) % NUM_SUB_FEATURES\n",
    "    sub_encoding[sub_index] = 1 if sub_index % 2 else -1\n",
    "    tfidf_features = vectorizer.transform([comment[\"body\"]]).toarray().flatten()\n",
    "    return [1, create_time, score, ups] + sub_encoding + tfidf_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(comment) for comment in train_data]\n",
    "with open(\"X_train.pkl\", \"wb\") as f:\n",
    "    import pickle\n",
    "    pickle.dump(X_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [class_indices[comment[\"author\"]] for comment in train_data]\n",
    "with open(\"y_train.pkl\", \"wb\") as f:\n",
    "    import pickle\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [feature(comment) for comment in test_data]\n",
    "with open(\"X_test.pkl\", \"wb\") as f:\n",
    "    import pickle\n",
    "    pickle.dump(X_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [class_indices[comment[\"author\"]] for comment in test_data]\n",
    "with open(\"y_test.pkl\", \"wb\") as f:\n",
    "    import pickle\n",
    "    pickle.dump(y_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
